# Does Domain-Specific Retrieval Augmented Generation Help LLMs Answer Consumer Health Questions?

This repo contains the code for the paper _Does Domain-Specific Retrieval Augmented Generation Help LLMs Answer Consumer Health Questions?_

## Requirements

Requirements to run code in this repo are listed in
```bash
requirements.txt
```

## Dataset

### Step 1: Download Dataset

1. Please download the following dataset manually from its repository: MedQuAD question-answer dataset: [MedQuAD](https://github.com/abachaa/MedQuAD) ([NIDDK](https://github.com/abachaa/MedQuAD/tree/master/5_NIDDK_QA) dataset)

### Document Corpus Extraction

- To extract the MedQuAD document corpus from webpages (n=151 for NIDDK):
First, for all invalid URLs from MedQuAD, get most recent valid URLS from WayBack Machine:

```bash
python medquad_data_collection/get_all_niddk_urls.py
```

Then with all valid urls, download all documents:

```bash
python medquad_data_collection/pull_docs_total.py
```

Result of this process is stored in ```/medquad_data_collection/MEDQUAD_DOCS```.

## Step 2: Answer Generation

For the MedQuAD NIDDK dataset, you can generate answers using two approaches with the code in this repository:

### A. Vanilla LLM Approach (No RAG)

To run vanilla LLM question-answering on all MedQuAD NIDDK questions, run the corresponding script for each model (e.g., for Llama-3-70B-Instruct):

```bash
python answer_generation/vanilla_answer_generation/Meta-Llama-3-70B-Instruct.py
python answer_generation/vanilla_answer_generation/Meta-Llama-3-8B-Instruct.py
python answer_generation/vanilla_answer_generation/Mistral-8x7B-Instruct.py
```

### B. RAG Approach

To prepare the MedQuAD knowledge base for retrieval and perform RAG on all MedQuAD NIDDK questions:

1. Preprocess and Vectorize Documents: First, preprocess, chunk, and vectorize the NIDDK documents. Use a randomly sampled set of questions for preprocessing tuning, for example: ```sample12_NEWURLS_MEDQUAD_NIDDK_QA_DATASET.csv```:

```bash
python answer_generation/RAG_answer_generation/preprocessing/main_rag_prepare.py
```

2. Generate Answers with RAG: Second, perform RAG answer generation with the created ChromaDB vector database (retreival parameters can be adjusted in the script):

```bash
python answer_generation/RAG_answer_generation/corpusaware_exp_medical_rag_qa.py
```

## Step 3: Answer Evalauation

To evaluate results of answers generated by vanilla LLMs and RAG:
**Quantitative Evaluation**:
Perform quantitative evaluation for RAG and vanilla LLM answers, respectively:

```bash
python answer_generation/metric_evaluation.py
```

**LLM-as-a-judge Evaluation**:
Perform qualitative LLM-as-a-judge evaluation:

```bash
python LLM_judge/LLM_judge_RAG.py
```

```bash
python LLM_judge/LLM_judge_vanilla.py
```

**Summaryize LLM-as-a-judge Results**
Summarize LLM judge results across vanilla and RAG, with optional args ```--complete-only``` ```--intersection-only``` (see script for details):

```bash
python LLM_judge/JUDGE_RESULTS/summarize_judge.py 
```

## Step 4. Human-LLM Agreement Analysis

Given blinded clinician annotations of LLM-only and RAG-generated answers (n=66) in ```LLM_judge/clinician_evaluation/A1_H2H_ANNOTATION_REQUEST_COMPLETED.xlsx```, compare LLM-judge vs. clinician's annotations for validation. Calculated results using a clinical annotator were calculated across LLMs, RAG, and NIDDK question categories.

## Repository Structure

```text
├── ANALYSIS/
│   └── RAG_total/...
├── LLM_judge/
│   ├── JUDGE_RESULTS/...
│   ├── LLM_judge_RAG.py
│   ├── LLM_judge_vanilla.py
│   └── clinician_evaluation/...
├── answer_generation/
│   ├── RAG_answer_generation/...
│   ├── vanilla_answer_generation/...
│   └── metric_evaluation.py
├── medquad_data_collection/
│   ├── MEDQUAD_DOCS/
│   ├── get_all_niddk_urls.py
│   └── pull_docs_total.py
├── medquad_results_vanilla_noRAG/...
└── requirements.txt
```

## Evaluation Framework

This repository implements a comprehensive evaluation framework that includes:

- **Multiple Model Support**: Supports various state-of-the-art language models (Llama, Mixtral, Meditron)
- **Dual Evaluation Approach**: Combines automated metrics with human expert evaluation
- **RAG vs Vanilla Comparison**: Direct comparison between retrieval-augmented and vanilla LLM approaches
- **Clinical Validation**: Includes clinician annotations for ground truth comparison
- **Comprehensive Metrics**: Evaluates scientific consensus, safety, empathy, bias, and other critical dimensions

## Citation

If you find this repository valuable for your research, we kindly request that you acknowledge our paper by citing it. We appreciate your consideration.

```text
@article{pmlr-fensore25,
  title = {Does Domain-Specific Retrieval Augmented Generation Help LLMs Answer Consumer Health Questions?},
  author = {Fensore, Chase M and Carrillo-Larco, Rodrigo M and Shah, Megha K and Ho, Joyce C},
  booktitle = {Proceedings of the 10th Machine Learning for Healthcare Conference},
  pages = {TBA},
  year = {2025},
  editor = {TBA},
  volume = {TBA},
  series = {Proceedings of Machine Learning Research},
  month = {TBA},
  publisher = {PMLR},
  pdf = {TBA},
  url = {TBA}
}
```
